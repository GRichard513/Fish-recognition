{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <a href=https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring/>The nature conservancy fisheries monitoring</a> </h1>\n",
    "\n",
    "<h2> NemoX </h2>\n",
    "\n",
    "\n",
    "<i> Raphaël Meudec, Guillaume Richard, Antoine SIMOULIN</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Introduction </h2>\n",
    "\n",
    "<p style=\"text-align: justify\"> Nearly half of the world depends on seafood for their main source of protein. In the Western and Central Pacific, where 60% of the world’s tuna is caught, illegal, unreported, and unregulated fishing practices are threatening marine ecosystems, global seafood supplies and local livelihoods. The Nature Conservancy is working with local, regional and global partners to preserve this fishery for the future. </p>\n",
    "\n",
    "<p style=\"text-align: justify\"> Currently, the Conservancy is looking to the future by using cameras to dramatically scale the monitoring of fishing activities to fill critical science and compliance monitoring data gaps. Although these electronic monitoring systems work well and are ready for wider deployment, the amount of raw data produced is cumbersome and expensive to process manually. </p>\n",
    "\n",
    "<p style=\"text-align: justify\"> The Conservancy is inviting the Kaggle community to develop algorithms to automatically detect and classify species of tunas, sharks and more that fishing boats catch, which will accelerate the video review process. Faster review and more reliable data will enable countries to reallocate human capital to management and enforcement activities which will have a positive impact on conservation and our planet. </p>\n",
    "\n",
    "<p style=\"text-align: justify\"> Machine learning has the ability to transform what we know about our oceans and how we manage them. You can be part of the solution. </p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Requirements </h2>\n",
    "\n",
    "* numpy=1.11.3  \n",
    "* matplotlib=1.5.3 \n",
    "* scikit-learn=0.18.1\n",
    "* pandas=0.18.1\n",
    "* seaborn=0.7.1\n",
    "* opencv=3.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1989)\n",
    "import matplotlib as mlt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import glob\n",
    "from scipy.misc import *\n",
    "import skimage.measure as sm\n",
    "# import progressbar\n",
    "import multiprocessing\n",
    "import random\n",
    "from subprocess import check_output\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "new_style = {'grid': False}\n",
    "plt.rc('axes', **new_style)\n",
    "\n",
    "# sklearn\n",
    "import sklearn as skt\n",
    "from sklearn import cluster\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D, AveragePooling2D\n",
    "from keras.optimizers import SGD, Adagrad\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras.constraints import maxnorm\n",
    "from keras import __version__ as keras_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '/Users/Antoine/anaconda3/lib/python35.zip',\n",
       " '/Users/Antoine/anaconda3/lib/python3.5',\n",
       " '/Users/Antoine/anaconda3/lib/python3.5/plat-darwin',\n",
       " '/Users/Antoine/anaconda3/lib/python3.5/lib-dynload',\n",
       " '/Users/Antoine/.local/lib/python3.5/site-packages',\n",
       " '/Users/Antoine/.local/lib/python3.5/site-packages/xgboost-0.6-py3.5.egg',\n",
       " '/Users/Antoine/anaconda3/lib/python3.5/site-packages',\n",
       " '/Users/Antoine/anaconda3/lib/python3.5/site-packages/Sphinx-1.4.6-py3.5.egg',\n",
       " '/Users/Antoine/anaconda3/lib/python3.5/site-packages/aeosa',\n",
       " '/Users/Antoine/anaconda3/lib/python3.5/site-packages/setuptools-27.2.0-py3.5.egg',\n",
       " '/Users/Antoine/anaconda3/lib/python3.5/site-packages/IPython/extensions',\n",
       " '/Users/Antoine/.ipython']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Loading data </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../input/\" directory.\n",
    "\n",
    "cat={'ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT'}\n",
    "# Function to show 4 images\n",
    "def show_four(imgs, title):\n",
    "    #select_imgs = [np.random.choice(imgs) for _ in range(4)]\n",
    "    select_imgs = [imgs[np.random.choice(len(imgs))] for _ in range(4)]\n",
    "    _, ax = plt.subplots(1, 4, sharex='col', sharey='row', figsize=(20, 3))\n",
    "    plt.suptitle(title, size=20)\n",
    "    for i, img in enumerate(select_imgs):\n",
    "        ax[i].imshow(img)\n",
    "\n",
    "# Function to show 8 images\n",
    "def show_eight(imgs, title):\n",
    "    select_imgs = [imgs[np.random.choice(len(imgs))] for _ in range(8)]\n",
    "    _, ax = plt.subplots(2, 4, sharex='col', sharey='row', figsize=(20, 6))\n",
    "    plt.suptitle(title, size=20)\n",
    "    for i, img in enumerate(select_imgs):\n",
    "        ax[i // 4, i % 4].imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    nb=30\n",
    "\n",
    "    # Data loading\n",
    "    train_files={}\n",
    "    train={}\n",
    "    for c in cat:\n",
    "        train_files[c] = sorted(glob.glob('./train/'+c+'/*.jpg'), key=lambda x: random.random())[:nb]\n",
    "        train[c] = np.array([imread(img) for img in train_files[c]])\n",
    "        print('Length of train %s: %i'%(c,len(train[c])))\n",
    "        show_four(train[c],c)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get image and resize it\n",
    "def get_im_cv2(path):\n",
    "    img = imread(path)\n",
    "    resized = imresize(img, (64, 64))\n",
    "    return resized\n",
    "\n",
    "#Load train data\n",
    "def load_train():\n",
    "    X_train = []\n",
    "    X_train_id = []\n",
    "    y_train = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    print('Read train images')\n",
    "    folders = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n",
    "    for fld in folders:\n",
    "        index = folders.index(fld)\n",
    "        print('Load folder {} (Index: {})'.format(fld, index))\n",
    "        path = os.path.join('./', 'train', fld, '*.jpg')\n",
    "        files = glob.glob(path)\n",
    "        for fl in files:\n",
    "            flbase = os.path.basename(fl)\n",
    "            img = get_im_cv2(fl)\n",
    "            X_train.append(img)\n",
    "            X_train_id.append(flbase)\n",
    "            y_train.append(index)\n",
    "\n",
    "    print('Read train data time: {} seconds'.format(round(time.time() - start_time, 2)))\n",
    "    return X_train, y_train, X_train_id\n",
    "\n",
    "\n",
    "#Load test data\n",
    "def load_test():\n",
    "    path = os.path.join('./', 'test_stg1', '*.jpg')\n",
    "    files = sorted(glob.glob(path))\n",
    "\n",
    "    X_test = []\n",
    "    X_test_id = []\n",
    "    for fl in files:\n",
    "        flbase = os.path.basename(fl)\n",
    "        img = get_im_cv2(fl)\n",
    "        X_test.append(img)\n",
    "        X_test_id.append(flbase)\n",
    "\n",
    "    return X_test, X_test_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creates the submission file\n",
    "def create_submission(predictions, test_id, info):\n",
    "    result1 = pd.DataFrame(predictions, columns=['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT'])\n",
    "    result1.loc[:, 'image'] = pd.Series(test_id, index=result1.index)\n",
    "    now = datetime.datetime.now()\n",
    "    sub_file = 'submission_' + info + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "    result1.to_csv(sub_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_and_normalize_train_data():\n",
    "    train_data, train_target, train_id = load_train()\n",
    "\n",
    "    print('Convert to numpy...')\n",
    "    train_data = np.array(train_data, dtype=np.uint8)\n",
    "    train_target = np.array(train_target, dtype=np.uint8)\n",
    "\n",
    "    print('Reshape...')\n",
    "    print(train_data)\n",
    "    train_data = train_data.transpose((0, 3, 1, 2))\n",
    "\n",
    "    print('Convert to float...')\n",
    "    train_data = train_data.astype('float32')\n",
    "    train_data = train_data / 255\n",
    "    train_target = np_utils.to_categorical(train_target, 8)\n",
    "\n",
    "    print('Train shape:', train_data.shape)\n",
    "    print(train_data.shape[0], 'train samples')\n",
    "    return train_data, train_target, train_id\n",
    "\n",
    "def read_and_normalize_test_data():\n",
    "    start_time = time.time()\n",
    "    test_data, test_id = load_test()\n",
    "\n",
    "    test_data = np.array(test_data, dtype=np.uint8)\n",
    "    test_data = test_data.transpose((0, 3, 1, 2))\n",
    "\n",
    "    test_data = test_data.astype('float32')\n",
    "    test_data = test_data / 255\n",
    "\n",
    "    print('Test shape:', test_data.shape)\n",
    "    print(test_data.shape[0], 'test samples')\n",
    "    print('Read and process test data time: {} seconds'.format(round(time.time() - start_time, 2)))\n",
    "    return test_data, test_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dict_to_list(d):\n",
    "    ret = []\n",
    "    for i in d.items():\n",
    "        ret.append(i[1])\n",
    "    return ret\n",
    "\n",
    "\n",
    "def merge_several_folds_mean(data, nfolds):\n",
    "    a = np.array(data[0])\n",
    "    for i in range(1, nfolds):\n",
    "        a += np.array(data[i])\n",
    "    a /= nfolds\n",
    "    return a.tolist()\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1, 1), input_shape=(3, 64, 64), dim_ordering='th'))\n",
    "    model.add(Convolution2D(8, 3, 3, activation='relu', dim_ordering='th', init='he_uniform'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), dim_ordering='th'))\n",
    "    model.add(ZeroPadding2D((1, 1), dim_ordering='th'))\n",
    "    model.add(Convolution2D(16, 3, 3, activation='relu', dim_ordering='th', init='he_uniform'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), dim_ordering='th'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(96, activation='relu',init='he_uniform'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(24, activation='relu',init='he_uniform'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(8, activation='softmax'))\n",
    "\n",
    "    sgd = SGD(lr=1e-2, decay=1e-4, momentum=0.88, nesterov=False)\n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_validation_predictions(train_data, predictions_valid):\n",
    "    pv = []\n",
    "    for i in range(len(train_data)):\n",
    "        pv.append(predictions_valid[i])\n",
    "    return pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version: 1.2.1\n",
      "Read train images\n",
      "Load folder ALB (Index: 0)\n",
      "Load folder BET (Index: 1)\n",
      "Load folder DOL (Index: 2)\n",
      "Load folder LAG (Index: 3)\n",
      "Load folder NoF (Index: 4)\n",
      "Load folder OTHER (Index: 5)\n",
      "Load folder SHARK (Index: 6)\n",
      "Load folder YFT (Index: 7)\n",
      "Read train data time: 196.64 seconds\n",
      "Convert to numpy...\n",
      "Reshape...\n",
      "[[[[205 205 204]\n",
      "   [204 204 203]\n",
      "   [208 209 208]\n",
      "   ..., \n",
      "   [156 132 120]\n",
      "   [129 113 108]\n",
      "   [104  97  94]]\n",
      "\n",
      "  [[156 161 158]\n",
      "   [140 145 148]\n",
      "   [159 162 164]\n",
      "   ..., \n",
      "   [195 165 145]\n",
      "   [187 160 144]\n",
      "   [144 130 124]]\n",
      "\n",
      "  [[ 99  80  83]\n",
      "   [ 81  83  93]\n",
      "   [101 106 110]\n",
      "   ..., \n",
      "   [197 167 146]\n",
      "   [208 179 157]\n",
      "   [206 182 164]]\n",
      "\n",
      "  ..., \n",
      "  [[ 85  96 102]\n",
      "   [ 91 102 115]\n",
      "   [119 133 147]\n",
      "   ..., \n",
      "   [ 60  74  89]\n",
      "   [ 60  88 113]\n",
      "   [ 86 123 152]]\n",
      "\n",
      "  [[ 77  88  96]\n",
      "   [ 86  98 112]\n",
      "   [119 134 148]\n",
      "   ..., \n",
      "   [ 60  71  82]\n",
      "   [ 69  87 105]\n",
      "   [ 51  79 120]]\n",
      "\n",
      "  [[ 64  74  86]\n",
      "   [ 82  95 111]\n",
      "   [116 132 146]\n",
      "   ..., \n",
      "   [ 47  62  74]\n",
      "   [ 65  78  91]\n",
      "   [ 55  68  88]]]\n",
      "\n",
      "\n",
      " [[[205 205 203]\n",
      "   [204 204 203]\n",
      "   [208 209 208]\n",
      "   ..., \n",
      "   [ 88 102 112]\n",
      "   [116 127 125]\n",
      "   [155 163 158]]\n",
      "\n",
      "  [[144 149 155]\n",
      "   [143 149 158]\n",
      "   [150 160 173]\n",
      "   ..., \n",
      "   [ 84 101 116]\n",
      "   [ 74  91 104]\n",
      "   [ 72  89 100]]\n",
      "\n",
      "  [[ 86  68  83]\n",
      "   [ 87  90 107]\n",
      "   [ 94 108 130]\n",
      "   ..., \n",
      "   [ 87 105 119]\n",
      "   [ 79  97 111]\n",
      "   [ 77  95 109]]\n",
      "\n",
      "  ..., \n",
      "  [[ 97  94  90]\n",
      "   [107 102  99]\n",
      "   [109 107 108]\n",
      "   ..., \n",
      "   [ 97  99  90]\n",
      "   [ 78  80  72]\n",
      "   [ 72  74  66]]\n",
      "\n",
      "  [[ 91  88  84]\n",
      "   [102  97  94]\n",
      "   [104 101 101]\n",
      "   ..., \n",
      "   [105 106  99]\n",
      "   [ 88  89  84]\n",
      "   [ 72  73  66]]\n",
      "\n",
      "  [[ 84  80  77]\n",
      "   [ 97  92  89]\n",
      "   [103  99  98]\n",
      "   ..., \n",
      "   [108 108 103]\n",
      "   [100 100  98]\n",
      "   [ 73  74  67]]]\n",
      "\n",
      "\n",
      " [[[230 229 214]\n",
      "   [228 168 170]\n",
      "   [235 115 132]\n",
      "   ..., \n",
      "   [ 92 110 117]\n",
      "   [114 131 138]\n",
      "   [136 153 160]]\n",
      "\n",
      "  [[245 171 174]\n",
      "   [249 112 126]\n",
      "   [225 106 115]\n",
      "   ..., \n",
      "   [ 92 109 116]\n",
      "   [ 95 112 119]\n",
      "   [ 98 115 122]]\n",
      "\n",
      "  [[241 156 161]\n",
      "   [219 108 114]\n",
      "   [137  86  88]\n",
      "   ..., \n",
      "   [ 83 100 106]\n",
      "   [122 139 146]\n",
      "   [121 138 145]]\n",
      "\n",
      "  ..., \n",
      "  [[254 254 254]\n",
      "   [254 254 254]\n",
      "   [255 255 255]\n",
      "   ..., \n",
      "   [ 53  74  79]\n",
      "   [ 56  78  84]\n",
      "   [ 63  86  92]]\n",
      "\n",
      "  [[254 254 253]\n",
      "   [255 255 255]\n",
      "   [254 254 254]\n",
      "   ..., \n",
      "   [ 54  75  80]\n",
      "   [ 69  91  97]\n",
      "   [ 79 102 108]]\n",
      "\n",
      "  [[247 245 243]\n",
      "   [255 255 255]\n",
      "   [249 249 249]\n",
      "   ..., \n",
      "   [ 73  94  99]\n",
      "   [ 86 108 113]\n",
      "   [ 88 110 116]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[164 169 166]\n",
      "   [166 169 169]\n",
      "   [166 167 168]\n",
      "   ..., \n",
      "   [212 207 192]\n",
      "   [217 209 196]\n",
      "   [216 211 197]]\n",
      "\n",
      "  [[162 167 164]\n",
      "   [166 169 168]\n",
      "   [161 162 163]\n",
      "   ..., \n",
      "   [214 207 193]\n",
      "   [216 209 195]\n",
      "   [216 211 197]]\n",
      "\n",
      "  [[164 169 165]\n",
      "   [163 167 164]\n",
      "   [158 160 160]\n",
      "   ..., \n",
      "   [216 205 193]\n",
      "   [217 206 194]\n",
      "   [216 207 195]]\n",
      "\n",
      "  ..., \n",
      "  [[ 87  92 100]\n",
      "   [ 86  89  98]\n",
      "   [ 87  90  99]\n",
      "   ..., \n",
      "   [138 109  79]\n",
      "   [135 105  77]\n",
      "   [127 100  71]]\n",
      "\n",
      "  [[ 82  87  95]\n",
      "   [ 83  86  95]\n",
      "   [ 82  85  94]\n",
      "   ..., \n",
      "   [137 108  77]\n",
      "   [130 100  72]\n",
      "   [124  96  67]]\n",
      "\n",
      "  [[ 71  80  86]\n",
      "   [ 81  85  93]\n",
      "   [ 84  87  96]\n",
      "   ..., \n",
      "   [138 109  77]\n",
      "   [131 102  73]\n",
      "   [122  95  66]]]\n",
      "\n",
      "\n",
      " [[[ 53  57  56]\n",
      "   [ 64  70  73]\n",
      "   [ 63  65  74]\n",
      "   ..., \n",
      "   [109 124 135]\n",
      "   [ 91 112 128]\n",
      "   [103 122 137]]\n",
      "\n",
      "  [[ 55  58  57]\n",
      "   [ 77  77  77]\n",
      "   [ 92  92  92]\n",
      "   ..., \n",
      "   [110 125 136]\n",
      "   [105 123 135]\n",
      "   [106 123 138]]\n",
      "\n",
      "  [[ 52  54  54]\n",
      "   [ 78  78  78]\n",
      "   [118 118 117]\n",
      "   ..., \n",
      "   [114 129 141]\n",
      "   [110 127 137]\n",
      "   [104 121 137]]\n",
      "\n",
      "  ..., \n",
      "  [[ 38  40  45]\n",
      "   [ 46  46  55]\n",
      "   [ 41  42  48]\n",
      "   ..., \n",
      "   [  3  26  51]\n",
      "   [  3  29  55]\n",
      "   [  2  28  55]]\n",
      "\n",
      "  [[ 34  37  41]\n",
      "   [ 44  45  53]\n",
      "   [ 39  40  46]\n",
      "   ..., \n",
      "   [  3  30  54]\n",
      "   [  1  27  53]\n",
      "   [  1  28  55]]\n",
      "\n",
      "  [[ 30  34  37]\n",
      "   [ 38  40  47]\n",
      "   [ 44  45  52]\n",
      "   ..., \n",
      "   [  6  33  56]\n",
      "   [  1  26  50]\n",
      "   [  3  30  57]]]\n",
      "\n",
      "\n",
      " [[[205 205 204]\n",
      "   [204 204 203]\n",
      "   [208 209 208]\n",
      "   ..., \n",
      "   [ 82 104 110]\n",
      "   [ 88 100 108]\n",
      "   [103 107 112]]\n",
      "\n",
      "  [[163 164 160]\n",
      "   [146 152 155]\n",
      "   [163 163 164]\n",
      "   ..., \n",
      "   [117 130 133]\n",
      "   [101 109 116]\n",
      "   [104 108 112]]\n",
      "\n",
      "  [[107  84  86]\n",
      "   [ 89  94 103]\n",
      "   [111 106 106]\n",
      "   ..., \n",
      "   [137 145 144]\n",
      "   [105 112 121]\n",
      "   [102 104 110]]\n",
      "\n",
      "  ..., \n",
      "  [[ 92 110 120]\n",
      "   [101 118 130]\n",
      "   [110 123 126]\n",
      "   ..., \n",
      "   [ 41  67  98]\n",
      "   [ 24  71 116]\n",
      "   [ 43 104 160]]\n",
      "\n",
      "  [[ 89 107 117]\n",
      "   [ 99 117 127]\n",
      "   [110 124 126]\n",
      "   ..., \n",
      "   [ 47  64  89]\n",
      "   [ 47  78 110]\n",
      "   [ 30  77 150]]\n",
      "\n",
      "  [[ 90 108 117]\n",
      "   [ 99 117 127]\n",
      "   [112 125 127]\n",
      "   ..., \n",
      "   [ 43  63  91]\n",
      "   [ 52  71  96]\n",
      "   [ 42  60  98]]]]\n",
      "Convert to float...\n",
      "Train shape: (3777, 3, 64, 64)\n",
      "3777 train samples\n",
      "Start KFold number 1 from 3\n",
      "Split train:  2518 2518\n",
      "Split valid:  1259 1259\n",
      "Train on 2518 samples, validate on 1259 samples\n",
      "Epoch 1/8\n",
      "26s - loss: 1.6506 - val_loss: 1.4998\n",
      "Epoch 2/8\n",
      "26s - loss: 1.4344 - val_loss: 1.3440\n",
      "Epoch 3/8\n",
      "26s - loss: 1.2860 - val_loss: 1.1784\n",
      "Epoch 4/8\n",
      "25s - loss: 1.1606 - val_loss: 1.0398\n",
      "Epoch 5/8\n",
      "28s - loss: 0.9581 - val_loss: 0.8006\n",
      "Epoch 6/8\n",
      "26s - loss: 0.8307 - val_loss: 0.6419\n",
      "Epoch 7/8\n",
      "26s - loss: 0.6715 - val_loss: 0.5269\n",
      "Epoch 8/8\n",
      "26s - loss: 0.5821 - val_loss: 0.5269\n",
      "Score log_loss:  0.526927316059\n",
      "Start KFold number 2 from 3\n",
      "Split train:  2518 2518\n",
      "Split valid:  1259 1259\n",
      "Train on 2518 samples, validate on 1259 samples\n",
      "Epoch 1/8\n",
      "27s - loss: 1.6887 - val_loss: 1.5685\n",
      "Epoch 2/8\n",
      "27s - loss: 1.4932 - val_loss: 1.3738\n",
      "Epoch 3/8\n",
      "26s - loss: 1.3239 - val_loss: 1.1964\n",
      "Epoch 4/8\n",
      "26s - loss: 1.1728 - val_loss: 1.0773\n",
      "Epoch 5/8\n",
      "26s - loss: 1.0499 - val_loss: 0.9456\n",
      "Epoch 6/8\n",
      "26s - loss: 0.9377 - val_loss: 0.7278\n",
      "Epoch 7/8\n",
      "26s - loss: 0.7798 - val_loss: 0.6552\n",
      "Epoch 8/8\n",
      "26s - loss: 0.6936 - val_loss: 0.5670\n",
      "Score log_loss:  0.566963381603\n",
      "Start KFold number 3 from 3\n",
      "Split train:  2518 2518\n",
      "Split valid:  1259 1259\n",
      "Train on 2518 samples, validate on 1259 samples\n",
      "Epoch 1/8\n",
      "26s - loss: 1.7251 - val_loss: 1.5793\n",
      "Epoch 2/8\n",
      "26s - loss: 1.5400 - val_loss: 1.4460\n",
      "Epoch 3/8\n",
      "26s - loss: 1.3507 - val_loss: 1.2337\n",
      "Epoch 4/8\n",
      "27s - loss: 1.1936 - val_loss: 1.0984\n",
      "Epoch 5/8\n",
      "29s - loss: 1.0357 - val_loss: 1.0629\n",
      "Epoch 6/8\n",
      "29s - loss: 0.9384 - val_loss: 0.7698\n",
      "Epoch 7/8\n",
      "29s - loss: 0.8335 - val_loss: 0.7388\n",
      "Epoch 8/8\n",
      "30s - loss: 0.7260 - val_loss: 0.6592\n",
      "Score log_loss:  0.659185519899\n",
      "Log_loss train independent avg:  0.584358739187\n",
      "Start KFold number 1 from 3\n",
      "Test shape: (1000, 3, 64, 64)\n",
      "1000 test samples\n",
      "Read and process test data time: 52.06 seconds\n",
      "Start KFold number 2 from 3\n",
      "Test shape: (1000, 3, 64, 64)\n",
      "1000 test samples\n",
      "Read and process test data time: 49.79 seconds\n",
      "Start KFold number 3 from 3\n",
      "Test shape: (1000, 3, 64, 64)\n",
      "1000 test samples\n",
      "Read and process test data time: 49.75 seconds\n"
     ]
    }
   ],
   "source": [
    "def run_cross_validation_create_models(nfolds=10):\n",
    "    # input image dimensions\n",
    "    batch_size = 32\n",
    "    nb_epoch = 8\n",
    "    random_state = 51\n",
    "    first_rl = 96\n",
    "\n",
    "    train_data, train_target, train_id = read_and_normalize_train_data()\n",
    "\n",
    "    yfull_train = dict()\n",
    "    kf = KFold(len(train_id), n_folds=nfolds, shuffle=True, random_state=random_state)\n",
    "    num_fold = 0\n",
    "    sum_score = 0\n",
    "    models = []\n",
    "    for train_index, test_index in kf:\n",
    "        model = create_model()\n",
    "        X_train = train_data[train_index]\n",
    "        Y_train = train_target[train_index]\n",
    "        X_valid = train_data[test_index]\n",
    "        Y_valid = train_target[test_index]\n",
    "\n",
    "        num_fold += 1\n",
    "        print('Start KFold number {} from {}'.format(num_fold, nfolds))\n",
    "        print('Split train: ', len(X_train), len(Y_train))\n",
    "        print('Split valid: ', len(X_valid), len(Y_valid))\n",
    "\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=3, verbose=0),\n",
    "        ]\n",
    "        model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "              shuffle=True, verbose=2, validation_data=(X_valid, Y_valid),\n",
    "              callbacks=callbacks)\n",
    "\n",
    "        predictions_valid = model.predict(X_valid.astype('float32'), batch_size=batch_size, verbose=2)\n",
    "        score = log_loss(Y_valid, predictions_valid)\n",
    "        print('Score log_loss: ', score)\n",
    "        sum_score += score*len(test_index)\n",
    "\n",
    "        # Store valid predictions\n",
    "        for i in range(len(test_index)):\n",
    "            yfull_train[test_index[i]] = predictions_valid[i]\n",
    "\n",
    "        models.append(model)\n",
    "\n",
    "    score = sum_score/len(train_data)\n",
    "    print(\"Log_loss train independent avg: \", score)\n",
    "\n",
    "    info_string = '_' + str(np.round(score,3)) + '_flds_' + str(nfolds) + '_eps_' + str(nb_epoch) + '_fl_' + str(first_rl)\n",
    "    return info_string, models\n",
    "\n",
    "\n",
    "def run_cross_validation_process_test(info_string, models):\n",
    "    batch_size = 24\n",
    "    num_fold = 0\n",
    "    yfull_test = []\n",
    "    test_id = []\n",
    "    nfolds = len(models)\n",
    "\n",
    "    for i in range(nfolds):\n",
    "        model = models[i]\n",
    "        num_fold += 1\n",
    "        print('Start KFold number {} from {}'.format(num_fold, nfolds))\n",
    "        test_data, test_id = read_and_normalize_test_data()\n",
    "        test_prediction = model.predict(test_data, batch_size=batch_size, verbose=2)\n",
    "        yfull_test.append(test_prediction)\n",
    "\n",
    "    test_res = merge_several_folds_mean(yfull_test, nfolds)\n",
    "    info_string = 'loss_' + info_string \\\n",
    "                + '_folds_' + str(nfolds)\n",
    "    create_submission(test_res, test_id, info_string)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('Keras version: {}'.format(keras_version))\n",
    "    num_folds = 3\n",
    "    info_string, models = run_cross_validation_create_models(num_folds)\n",
    "    run_cross_validation_process_test(info_string, models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xfeatures2d_SIFT 0x102122510>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.xfeatures2d.SIFT_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
