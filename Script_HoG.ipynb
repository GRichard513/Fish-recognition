{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Check output----\n",
      " ALB\n",
      "BET\n",
      "DOL\n",
      "LAG\n",
      "NoF\n",
      "OTHER\n",
      "SHARK\n",
      "YFT\n",
      "\n",
      "\n",
      " ----Subdirectories----\n",
      " ['NoF', 'YFT', 'LAG', 'OTHER', 'SHARK', 'DOL', 'ALB', 'BET']\n",
      "('BET', 7, './train/BET/')\n",
      "('ALB', 6, './train/ALB/')\n",
      "('OTHER', 3, './train/OTHER/')\n",
      "('YFT', 1, './train/YFT/')\n",
      "('DOL', 5, './train/DOL/')\n",
      "('LAG', 2, './train/LAG/')\n",
      "('NoF', 0, './train/NoF/')\n",
      "('SHARK', 4, './train/SHARK/')\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn import svm\n",
    "import cv2\n",
    "import csv\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import glob\n",
    "from sklearn import cluster\n",
    "from scipy.misc import *\n",
    "import skimage.measure as sm\n",
    "# import progressbar\n",
    "import multiprocessing\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print('----Check output----\\n', check_output([\"ls\", \"./train/\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "dir_names = [d for d in listdir(\"./train/\") if not isfile(join(\"./train/\", d))]\n",
    "print('\\n ----Subdirectories----\\n', dir_names)\n",
    "\n",
    "#file_paths contains the file names for each category\n",
    "#key has format: (Category,CatNumber,Path)\n",
    "file_paths = {}\n",
    "class_num = 0\n",
    "cat_int = {}\n",
    "int_cat = {}\n",
    "prediction_output_list=[]\n",
    "test_names=[f for f in listdir(\"./test_stg1/\") if isfile(join(\"./test_stg1\", f))]\n",
    "for d in dir_names:\n",
    "    fnames = [f for f in listdir(\"./train/\"+d+\"/\") if isfile(join(\"./train/\"+d+\"/\", f))]\n",
    "    file_paths[(d, class_num, \"./train/\"+d+\"/\")] = fnames\n",
    "    cat_int[d]=class_num\n",
    "    int_cat[class_num]=d\n",
    "    class_num += 1\n",
    "for k in file_paths.keys():\n",
    "    print(k)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# General steps:\n",
    "# Extract feature from each file as HOG or similar... or SIFT... or Similar...\n",
    "# map each to feature space... and train some kind of classifier on that. SVM is a good choice.\n",
    "# do the same for each feature in test set...\n",
    "training_data = np.array([])\n",
    "training_labels = np.array([])\n",
    "\n",
    "for key in file_paths:\n",
    "    cat = key[0]\n",
    "    category = key[1]\n",
    "    directory_path = key[2]\n",
    "    file_list = file_paths[key]\n",
    "\n",
    "    # shuffle this list, so we get random examples\n",
    "    np.random.shuffle(file_list)\n",
    "\n",
    "    # Stop early, while testing, so it doesn't take FOR-EV-ER (FOR-EV-ER)\n",
    "    i = 0\n",
    "\n",
    "    # read in the file and get its SIFT features\n",
    "    for fname in file_list:\n",
    "        fpath = directory_path + fname\n",
    "        #print(fpath)\n",
    "        #print(\"Category = \" + str(category))\n",
    "        # extract features!\n",
    "        gray = cv2.imread(fpath,0)\n",
    "        img=cv2.imread(fpath,0)\n",
    "        gray = cv2.resize(gray, (400, 250))  # resize so we're always comparing same-sized images\n",
    "                                             # Could also make images larger/smaller\n",
    "                                             # to tune for greater accuracy / more speedd\n",
    "\n",
    "        \"\"\" My Choice: SIFT (Scale Invariant Feature Transform)\"\"\"\n",
    "        # However, this does not work on the Kaggle server\n",
    "        # because it's in a separate package in the opencv version used on the Kaggle server.\n",
    "        # This is a very robust method however, worth trying when it's reasonable to do so. \n",
    "        #detector = cv2.xfeatures2d.SIFT_create()\n",
    "        #kp1, des1 = detector.detectAndCompute(gray, None)\n",
    "        \n",
    "        #Image saving\n",
    "        #img=cv2.drawKeypoints(gray,kp1,img)\n",
    "        #cv2.imwrite('./pictures/keypoints/'+cat+'-'+fname,img)\n",
    "        \n",
    "        \"\"\" Another option that will work on Kaggle server is ORB\"\"\"\n",
    "        # find the keypoints with ORB\n",
    "        #kp = cv2.orb.detect(img,None)\n",
    "        # compute the descriptors with ORB\n",
    "        #kp1, des1 = cv2.orb.compute(img, kp)\n",
    "\n",
    "        \"\"\" Histogram of Gradients - often used to for detected people/animals in photos\"\"\"\n",
    "        # Havent' tried this one in the SVM yet, but here's how to get the HoG, using openCV\n",
    "        hog = cv2.HOGDescriptor()\n",
    "        des1 = hog.compute(gray)\n",
    "        # This is to make sure we have at least 100 keypoints to analyze\n",
    "        # could also duplicate a few features if needed to hit a higher value\n",
    "        #if len(kp1) < 100:\n",
    "        #    continue\n",
    "\n",
    "        # transform the data to float and shuffle all keypoints\n",
    "        # so we get a random sampling from each image\n",
    "        #des1 = des1.astype(np.float64)\n",
    "        #np.random.shuffle(des1)\n",
    "        #des1 = des1[0:100,:] # trim vector so all are same size\n",
    "        vector_data = des1#.reshape(1,12800) \n",
    "        list_data = des1.tolist()\n",
    "\n",
    "        # We need to concatenate ont the full list of features extracted from each image\n",
    "        if len(training_data) == 0:\n",
    "            training_data = np.append(training_data, vector_data)\n",
    "            training_data = training_data.reshape(np.shape(vector_data))\n",
    "        else:\n",
    "            training_data   = np.concatenate((training_data, vector_data), axis=1)\n",
    "\n",
    "        training_labels = np.append(training_labels,category)\n",
    "\n",
    "        # early stop\n",
    "        i += 1\n",
    "        if i > 50:\n",
    "            break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 400)\n",
      "(2600640, 1)\n"
     ]
    }
   ],
   "source": [
    "winSize = (64,64)\n",
    "blockSize = (16,16)\n",
    "blockStride = (8,8)\n",
    "cellSize = (8,8)\n",
    "nbins = 9\n",
    "derivAperture = 1\n",
    "winSigma = 4.\n",
    "histogramNormType = 0\n",
    "L2HysThreshold = 2.0000000000000001e-01\n",
    "gammaCorrection = 0\n",
    "nlevels = 64\n",
    "hog = cv2.HOGDescriptor(winSize,blockSize,blockStride,cellSize,nbins,derivAperture,winSigma,\n",
    "                        histogramNormType,L2HysThreshold,gammaCorrection,nlevels)\n",
    "hog = cv2.HOGDescriptor()\n",
    "#compute(img[, winStride[, padding[, locations]]]) -> descriptors\n",
    "winStride = (8,8)\n",
    "padding = (8,8)\n",
    "locations = ((10,20),)\n",
    "hist = hog.compute(gray)#,winStride,padding,locations)\n",
    "\n",
    "print(np.shape(gray))\n",
    "print(np.shape(hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (3679, 12800)\n",
      "Train labels shape: (3679,)\n"
     ]
    }
   ],
   "source": [
    "print('Train data shape:', np.shape(training_data))\n",
    "print('Train labels shape:', np.shape(training_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "        # Alright! Now we've got features extracted and labels\n",
    "        X = training_data\n",
    "        y = training_labels\n",
    "        y = y.reshape(y.shape[0],)\n",
    "\n",
    "        # Create and fit the SVM\n",
    "        # Fitting should take a few minutes\n",
    "        clf = svm.SVC(kernel='linear', C = 1.0, probability=True)\n",
    "        clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM Prediction:\n",
      "[ 6.]\n",
      "[[ 0.09772263  0.20247569  0.01739086  0.09344943  0.04264332  0.01031217\n",
      "   0.46582509  0.07018081]]\n"
     ]
    }
   ],
   "source": [
    "    # Now, extract one of the images and predict it\n",
    "    gray = cv2.imread('./test_stg1/img_00071.jpg', 0)  # Correct is LAG --> Class 3\n",
    "    img=cv2.imread('./test_stg1/img_00071.jpg', 0)\n",
    "    img=cv2.drawKeypoints(gray,kp1,img)\n",
    "\n",
    "    kp1, des1 = detector.detectAndCompute(gray, None)\n",
    "    \n",
    "    des1 = des1[0:100, :]   # trim vector so all are same size\n",
    "    vector_data = des1.reshape(1, 12800)\n",
    "\n",
    "    print(\"Linear SVM Prediction:\")\n",
    "    print(clf.predict(vector_data))        # prints highest probability class, only\n",
    "    cv2.imwrite('./pictures/keypoints-test/'+int_cat[clf.predict(vector_data)[0]]+'-img_00071.jpg',img)\n",
    "    print(clf.predict_proba(vector_data))  # shows all probabilities for each class. \n",
    "                                           #    need this for the competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "530\n",
      "540\n",
      "550\n",
      "560\n",
      "570\n",
      "580\n",
      "590\n",
      "600\n",
      "610\n",
      "620\n",
      "630\n",
      "640\n",
      "650\n",
      "660\n",
      "670\n",
      "680\n",
      "690\n",
      "700\n",
      "710\n",
      "720\n",
      "730\n",
      "740\n",
      "750\n",
      "760\n",
      "770\n",
      "780\n",
      "790\n",
      "800\n",
      "810\n",
      "820\n",
      "830\n",
      "840\n",
      "850\n",
      "860\n",
      "870\n",
      "880\n",
      "890\n",
      "900\n",
      "910\n",
      "920\n",
      "930\n",
      "940\n",
      "950\n",
      "960\n",
      "970\n",
      "980\n",
      "990\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "    # early stoppage...\n",
    "    # only do 10\n",
    "    i = 0\n",
    "    prediction_output_list=[]\n",
    "    for f in test_names:\n",
    "        file_name = \"./test_stg1/\" + f\n",
    "        #print(\"---Evaluating File at: \" + file_name)\n",
    "        if i%10==0:\n",
    "            print(i)\n",
    "        gray = cv2.imread(file_name, 0)\n",
    "        #print(gray)\n",
    "        gray = cv2.resize(gray, (400, 250))  # resize so we're always comparing same-sized images\n",
    "        kp1, des1 = detector.detectAndCompute(gray, None)\n",
    "\n",
    "        # ensure we have at least 100 keypoints to analyze\n",
    "        if len(kp1) < 100:\n",
    "            # and duplicate some points if necessary\n",
    "            current_len = len(kp1)\n",
    "            vectors_needed = 100 - current_len\n",
    "            repeated_vectors = des1[0:vectors_needed, :]\n",
    "            # concatenate repeats onto des1\n",
    "            while len(des1) < 100:\n",
    "                des1 = np.concatenate((des1, repeated_vectors), axis=0)\n",
    "            # duplicate data just so we can run the model.\n",
    "            des1[current_len:100, :] = des1[0:vectors_needed, :]\n",
    "\n",
    "        np.random.shuffle(des1)  # shuffle the vector so we get a representative sample\n",
    "        des1 = des1[0:100, :]   # trim vector so all are same size\n",
    "        vector_data = des1.reshape(1, 12800)\n",
    "        #print(\"Linear SVM Prediction:\")\n",
    "        #print(clf.predict(vector_data))\n",
    "        svm_prediction = clf.predict_proba(vector_data)\n",
    "        #print(svm_prediction)\n",
    "        \n",
    "        # format list for csv output\n",
    "        csv_output_list = []\n",
    "        csv_output_list.append(f)\n",
    "        for elem in svm_prediction:      \n",
    "            for value in elem:\n",
    "                csv_output_list.append(value)\n",
    "\n",
    "        # append filename to make sure we have right format to write to csv\n",
    "        #print(\"CSV Output List Formatted:\")\n",
    "        #print(csv_output_list)\n",
    "\n",
    "        # and append this file to the output_list (of lists)\n",
    "        prediction_output_list.append(csv_output_list)\n",
    "\n",
    "        # Uncomment to stop early\n",
    "        #if i > 10:\n",
    "        #    break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['img_01211.jpg', 0.20959866561786863, 0.18148281761244581, 0.031380833370168018, 0.063047982405357139, 0.034837293008693197, 0.02588047656980667, 0.3932368303982034, 0.060535101017457217], ['img_01371.jpg', 0.074426005010022264, 0.1913930969504597, 0.0033881815251937315, 0.083907244074230378, 0.0261945741080373, 0.017889014777113164, 0.53250447434851922, 0.070297409206424369], ['img_01847.jpg', 0.15011363255324109, 0.21491124095708508, 0.010937533552748034, 0.068730041346514725, 0.050715697502354946, 0.013332333360684642, 0.42692874448310364, 0.064330776244268029], ['img_01548.jpg', 0.12362203698598814, 0.24378210089724286, 0.0038552599029145749, 0.073551859880386428, 0.050049968921071032, 0.020605483045887538, 0.43973006439547496, 0.044803225971034137], ['img_00518.jpg', 0.20780249597354292, 0.15553881764499933, 0.004928935055606945, 0.082167945883271046, 0.045777425599947526, 0.01663673801049571, 0.44019634403434443, 0.046951297797792182]]\n"
     ]
    }
   ],
   "source": [
    "    # Write to csv\n",
    "    print(prediction_output_list[0:5])\n",
    "    #\"\"\"  Uncomment to write to your CSV. Can't do this on Kaggle server directly.\n",
    "    try:\n",
    "        with open(\"sift_and_svm_submission.csv\", \"w\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            f.flush()\n",
    "            headers = ['image', 'ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n",
    "            #headers= \"image,ALB,BET,DOL,LAG,NoF,OTHER,SHARK,YFT\"\n",
    "            writer.writerow(headers)\n",
    "            writer.writerows(prediction_output_list)\n",
    "    finally:\n",
    "        f.close()\n",
    "    #\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(prediction_output_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function drawKeypoints:\n",
      "\n",
      "drawKeypoints(...)\n",
      "    drawKeypoints(image, keypoints, outImage[, color[, flags]]) -> outImage\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cv2.drawKeypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
